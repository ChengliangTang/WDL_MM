{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This Jupyter notebook reproduces the results in Figure 4 and Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os, sys, time\n",
    "sys.path.append('../lib/')\n",
    "import WDL as wp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import matplotlib.ticker as tick\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X_raw = pd.read_csv('../data/processed/dat_X.csv').to_numpy()\n",
    "X = X_raw[20:].copy()\n",
    "#X_raw = X_raw[:, :3]\n",
    "## features: 'GHG', 'Volcanic', 'Solar', 'ENSO'\n",
    "Y = pd.read_csv('../data/processed/dat_Y.csv').to_numpy()\n",
    "loc_cv = pd.read_csv('../data/processed/dat_CV.csv').to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested cross validation\n",
    "n_dist = Y.shape[0]\n",
    "n_levs = 100\n",
    "n_fold = np.max(loc_cv) + 1\n",
    "q_vec = np.arange(1, n_levs) / n_levs\n",
    "## transform Y\n",
    "Q_mat = np.array([np.quantile(Y[i], q_vec) for i in range(n_dist)])\n",
    "Q_train = np.zeros((Q_mat.shape[0], Q_mat.shape[1], n_fold))\n",
    "Q_test = np.zeros(Q_mat.shape)\n",
    "\n",
    "K_list = [2, 3, 5]\n",
    "lr_list = [5e-2, 1e-2]\n",
    "n_iter = 1000\n",
    "## outer loop\n",
    "time_start = datetime.now()\n",
    "print('Start training:', time_start)\n",
    "for i in range(n_fold):\n",
    "    print('This is fold', str(i+1))\n",
    "    X_train = X[loc_cv != i]\n",
    "    Y_train = Q_mat[loc_cv != i]\n",
    "    X_test = X[loc_cv == i]\n",
    "    Y_test = Q_mat[loc_cv == i]\n",
    "    \n",
    "    n_test = Y_test.shape[0]\n",
    "    n_train = Y_train.shape[0]\n",
    "    \n",
    "    ## inner parameter selection\n",
    "    X_t_in, X_v_in, Y_t_in, Y_v_in = train_test_split(X_train, Y_train, test_size=0.25, random_state=2022)\n",
    "    par_combo = [(K, lr) for K in K_list for lr in lr_list]\n",
    "    loss_ = []\n",
    "    iters_ = []\n",
    "    for K_mix, lr in par_combo:\n",
    "        print(K_mix, lr)\n",
    "        res_init = wp.WDL(X_t_in, Y_t_in, X_v_in, Y_v_in,\n",
    "                          q_vec=q_vec, K=K_mix, max_iter=n_iter, warm_up=1, max_depth=1, \n",
    "                          patience=10, lr=lr, random_state=2022)\n",
    "        iters_.append(res_init['iter_best'])\n",
    "        loss_.append(res_init['val_loss'][res_init['iter_best']])\n",
    "        \n",
    "    ## choose the best params\n",
    "    K_best, lr_best = par_combo[np.argmin(np.array(loss_))]\n",
    "    iter_best = iters_[np.argmin(np.array(loss_))]\n",
    "    print('Best:', K_best, lr_best, iter_best)\n",
    "    ## retrain the model over the training set\n",
    "    res = wp.WDL(X_train, Y_train, X_test, Y_test, q_vec=q_vec, \n",
    "                 K=K_best, max_iter=iter_best, warm_up=1, \n",
    "                 max_depth=1, early_stop=False, lr=lr_best, random_state=2022)\n",
    "    \n",
    "    alpha_test = np.zeros((n_test, K_best))\n",
    "    mu_test = np.zeros((n_test, K_best))\n",
    "    sigma_test = np.zeros((n_test, K_best))\n",
    "    \n",
    "    alpha_train = np.zeros((n_train, K_best))\n",
    "    mu_train = np.zeros((n_train, K_best))\n",
    "    sigma_train = np.zeros((n_train, K_best))\n",
    "    \n",
    "    v_lr = np.array([1] + [lr_best] * iter_best)\n",
    "    for k in range(K_best):\n",
    "        alpha_test[:, k] = wp.pred_boost(X_test, res['alpha'][k], lr_=v_lr, n_term=iter_best)\n",
    "        mu_test[:, k] = wp.pred_boost(X_test, res['mu'][k], lr_=v_lr, n_term=iter_best)\n",
    "        sigma_test[:, k] = np.exp(wp.pred_boost(X_test, res['sigma'][k], lr_=v_lr, n_term=iter_best))\n",
    "        \n",
    "        alpha_train[:, k] = wp.pred_boost(X_train, res['alpha'][k], lr_=v_lr, n_term=iter_best)\n",
    "        mu_train[:, k] = wp.pred_boost(X_train, res['mu'][k], lr_=v_lr, n_term=iter_best)\n",
    "        sigma_train[:, k] = np.exp(wp.pred_boost(X_train, res['sigma'][k], lr_=v_lr, n_term=iter_best))\n",
    "    \n",
    "    pi_test = np.exp(alpha_test)\n",
    "    pi_test = (pi_test.T / np.sum(pi_test, axis=1)).T\n",
    "    Q_test[loc_cv == i] = [wp.qgmm1d(q_vec, mu_test[j], sigma_test[j], pi_test[j]) for j in range(n_test)]\n",
    "    \n",
    "    pi_train = np.exp(alpha_train)\n",
    "    pi_train = (pi_train.T / np.sum(pi_train, axis=1)).T\n",
    "    Q_train[loc_cv != i, :, i] = [wp.qgmm1d(q_vec, mu_train[j], sigma_train[j], pi_train[j]) for j in range(n_train)]\n",
    "    Q_train[loc_cv == i, :, i] = np.nan\n",
    "    \n",
    "print('Done!')\n",
    "print('Time:', datetime.now() - time_start )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the results\n",
    "RMSE = np.mean((Q_mat - Q_test)**2)\n",
    "var_y = np.mean((Q_mat - np.mean(Q_mat, axis=0))**2)\n",
    "R_sq = 1 - RMSE / var_y\n",
    "print('Test loss:', RMSE)\n",
    "print('Test R-squared:', R_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the training result\n",
    "pd.DataFrame(Q_test).to_csv('predictions/pred_WDL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrain the model over the entire data set\n",
    "lr_list = [1e-1, 5e-2, 1e-2]\n",
    "K_mix = 3\n",
    "n_iter = 1000\n",
    "\n",
    "X_t, X_v, y_t, y_v = train_test_split(X, Q_mat, test_size=0.25, random_state=2020)\n",
    "loss_ = []\n",
    "iters_ = []\n",
    "for lr in lr_list:\n",
    "    print('Learning rate:', lr)\n",
    "    res_init = wp.WDL(X_t, y_t, X_v, y_v,\n",
    "                      q_vec=q_vec, K=K_mix, max_iter=n_iter, warm_up=1, max_depth=1, \n",
    "                      patience=5, lr=lr, random_state=2020)\n",
    "    iters_.append(res_init['iter_best'])\n",
    "    loss_.append(res_init['val_loss'][res_init['iter_best']])\n",
    "## choose the best params\n",
    "lr_best = lr_list[np.argmin(np.array(loss_))]\n",
    "iter_best = iters_[np.argmin(np.array(loss_))]\n",
    "print('Best:', lr_best, iter_best)\n",
    "## retrain the model over the training set\n",
    "res = wp.WDL(X, Q_mat, X, Q_mat, q_vec=q_vec, \n",
    "             K=K_mix, max_iter=iter_best, warm_up=1, \n",
    "             max_depth=1, early_stop=False, lr=lr_best, random_state=2020)\n",
    "\n",
    "v_lr = np.array([1] + [lr_best] * iter_best)\n",
    "\n",
    "## choose evaluation resolution\n",
    "n_evals = 100\n",
    "q_evals = np.arange(1, n_evals+1) / (n_evals+1)\n",
    "## initialize the parameters\n",
    "mu_partial = np.zeros((4, n_evals, K_mix))\n",
    "sigma_partial = np.zeros((4, n_evals, K_mix))\n",
    "pi_partial = np.zeros((4, n_evals, K_mix))\n",
    "time_start = datetime.now()\n",
    "print('Start:', time_start)\n",
    "for i in range(4):\n",
    "    for j in range(n_evals):\n",
    "        X_c = X.copy()\n",
    "        X_c[:, i] = np.quantile(X[:, i], q_evals[j])\n",
    "        alpha_temp = np.zeros((n_dist, K_mix))\n",
    "        mu_temp = np.zeros((n_dist, K_mix))\n",
    "        sigma_temp = np.zeros((n_dist, K_mix))\n",
    "        for k in range(K_mix):\n",
    "            alpha_temp[:, k] = wp.pred_boost(X_c, res['alpha'][k], lr_=v_lr, n_term=n_iter)\n",
    "            mu_temp[:, k] = wp.pred_boost(X_c, res['mu'][k], lr_=v_lr, n_term=n_iter)\n",
    "            sigma_temp[:, k] = np.exp(wp.pred_boost(X_c, res['sigma'][k], lr_=v_lr, n_term=n_iter))\n",
    "        pi_temp = np.exp(alpha_temp)\n",
    "        pi_temp = (pi_temp.T / np.sum(pi_temp, axis=1)).T\n",
    "        ## save the result\n",
    "        mu_partial[i, j] = np.mean(mu_temp, axis=0)\n",
    "        sigma_partial[i, j] = np.mean(sigma_temp, axis=0)\n",
    "        pi_partial[i, j] = np.mean(pi_temp, axis=0)\n",
    "print('Time:', datetime.now() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create partial dependence plot (Quantile)\n",
    "n_evals = 20\n",
    "q_evals = np.arange(n_evals+1) / n_evals\n",
    "p_levs = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "res_mat =  np.zeros((4, (n_evals + 1), 5))\n",
    "time_start = datetime.now()\n",
    "print('Start:', time_start)\n",
    "for i in range(4):\n",
    "    for j in range(n_evals+1):\n",
    "        X_c = X.copy()\n",
    "        X_c[:, i] = np.quantile(X[:, i], q_evals[j])\n",
    "        alpha_pred = np.zeros((n_dist, K_mix))\n",
    "        mu_pred = np.zeros((n_dist, K_mix))\n",
    "        sigma_pred = np.zeros((n_dist, K_mix))\n",
    "        for k in range(K_mix):\n",
    "            alpha_pred[:, k] = wp.pred_boost(X_c, res['alpha'][k], lr_=v_lr, n_term=n_iter)\n",
    "            mu_pred[:, k] = wp.pred_boost(X_c, res['mu'][k], lr_=v_lr, n_term=n_iter)\n",
    "            sigma_pred[:, k] = np.exp(wp.pred_boost(X_c, res['sigma'][k], lr_=v_lr, n_term=n_iter))\n",
    "        pi_pred = np.exp(alpha_pred)\n",
    "        pi_pred = (pi_pred.T / np.sum(pi_pred, axis=1)).T\n",
    "        res_mat[i, j] = np.mean([wp.qgmm1d(p_levs, mu_pred[l], sigma_pred[l], pi_pred[l]) for l in range(n_dist)], axis=0)\n",
    "print('Time:', datetime.now() - time_start)\n",
    "# save as .npy\n",
    "np.save('predictions/res_WDL_qt.npy', res_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "## load radiative forcings\n",
    "df_rf = pd.read_csv('../data/raw/fullDat.csv', index_col=0)\n",
    "df_rf = df_rf[['Year', 'GHG', 'Volcanic', 'Solar', 'ENSO']]\n",
    "## load temperatures\n",
    "df_temp = pd.read_table('../data/raw/Complete_TAVG_daily.txt', sep=' ', header=None)\n",
    "df_temp = df_temp[[2, 5, 6]]\n",
    "df_temp.columns = ['YEAR', 'DAY', 'TEMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "## create a temperature dictionary: year - daily temperatures\n",
    "temp_dict = {}\n",
    "year_start = 1880\n",
    "year_end = 2012\n",
    "for i in range(df_temp.shape[0]):\n",
    "    year_now = df_temp['YEAR'].iloc[i]\n",
    "    if year_now < year_start or year_now > year_end:\n",
    "        continue\n",
    "    if year_now in temp_dict.keys():\n",
    "        temp_dict[year_now].append(df_temp['TEMP'].iloc[i])\n",
    "    else:\n",
    "        temp_dict[year_now] = [df_temp['TEMP'].iloc[i]]\n",
    "## filter out year by threshold\n",
    "THRESH = 355\n",
    "year_select = [year_now for year_now in temp_dict.keys() if len(temp_dict[year_now]) >= THRESH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PDF\n",
    "pi_train = np.exp(alpha_train)\n",
    "pi_train = (pi_train.T / np.sum(pi_train, axis=1)).T\n",
    "pi_test = np.exp(alpha_test)\n",
    "pi_test = (pi_test.T / np.sum(pi_test, axis=1)).T\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(22, 6), sharex=True, sharey=True)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        id_loc = i * 50 + j * 10 + 20\n",
    "        n_sample = len(Y[id_loc])\n",
    "        p_loc = np.linspace(-2, 2, 100)\n",
    "        pdf_V = wp.dgmm1d(p_loc, mu_test[id_loc], sigma_test[id_loc], pi_test[id_loc])\n",
    "        pdf_T = wp.dgmm1d(p_loc, mu_train[id_loc], sigma_train[id_loc], pi_train[id_loc])\n",
    "        ax[i, j].plot(p_loc, pdf_T, label='train', c='black', linestyle='-')\n",
    "        ax[i, j].plot(p_loc, pdf_V, label='test', c='black', linestyle='-.')\n",
    "        ax[i, j].set_title(str(year_select[id_loc]), fontsize=16) \n",
    "        ax[i, j].legend(loc='upper right', fontsize=16)\n",
    "        ax[i, j].hist(Y[id_loc], bins=15, density=True, edgecolor='black', color='white')\n",
    "        ax[i, j].tick_params(labelsize=15)\n",
    "        #ax[i, j].set_xlabel('Degrees Fahrenheit', fontsize=12)\n",
    "#plt.savefig('../output/Figure_04.pdf', bbox_inches='tight')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the PDF\n",
    "pi_train = np.exp(alpha_train)\n",
    "pi_train = (pi_train.T / np.sum(pi_train, axis=1)).T\n",
    "pi_test = np.exp(alpha_test)\n",
    "pi_test = (pi_test.T / np.sum(pi_test, axis=1)).T\n",
    "\n",
    "fig, ax = plt.subplots(9, 5, figsize=(18, 20), sharex=True, sharey=True)\n",
    "for i in range(9):\n",
    "    for j in range(5):\n",
    "        id_loc = i * 5 + j\n",
    "        n_sample = len(Y[id_loc])\n",
    "        p_loc = np.linspace(-2, 2, 100)\n",
    "        pdf_V = wp.dgmm1d(p_loc, mu_test[id_loc], sigma_test[id_loc], pi_test[id_loc])\n",
    "        pdf_T = wp.dgmm1d(p_loc, mu_train[id_loc], sigma_train[id_loc], pi_train[id_loc])\n",
    "        ax[i, j].plot(p_loc, pdf_T, label='train', c='black', linestyle='-')\n",
    "        ax[i, j].plot(p_loc, pdf_V, label='test', c='black', linestyle='-.')\n",
    "        ax[i, j].set_title(str(year_select[id_loc]), fontsize=16) \n",
    "        #ax[i, j].legend(loc='upper right', fontsize=16)\n",
    "        ax[i, j].hist(Y[id_loc], bins=15, density=True, edgecolor='black', color='white')\n",
    "        ax[i, j].tick_params(labelsize=15)\n",
    "        #ax[i, j].set_xlabel('Degrees Fahrenheit', fontsize=12)\n",
    "fig.tight_layout()\n",
    "#plt.savefig('../output/Figure_appendix_1.pdf', bbox_inches='tight')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(9, 5, figsize=(18, 20), sharex=True, sharey=True)\n",
    "for i in range(9):\n",
    "    for j in range(5):\n",
    "        id_loc = i * 5 + j + 45\n",
    "        n_sample = len(Y[id_loc])\n",
    "        p_loc = np.linspace(-2, 2, 100)\n",
    "        pdf_V = wp.dgmm1d(p_loc, mu_test[id_loc], sigma_test[id_loc], pi_test[id_loc])\n",
    "        pdf_T = wp.dgmm1d(p_loc, mu_train[id_loc], sigma_train[id_loc], pi_train[id_loc])\n",
    "        ax[i, j].plot(p_loc, pdf_T, label='train', c='black', linestyle='-')\n",
    "        ax[i, j].plot(p_loc, pdf_V, label='test', c='black', linestyle='-.')\n",
    "        ax[i, j].set_title(str(year_select[id_loc]), fontsize=16) \n",
    "        #ax[i, j].legend(loc='upper right', fontsize=16)\n",
    "        ax[i, j].hist(Y[id_loc], bins=15, density=True, edgecolor='black', color='white')\n",
    "        ax[i, j].tick_params(labelsize=15)\n",
    "        #ax[i, j].set_xlabel('Degrees Fahrenheit', fontsize=12)\n",
    "fig.tight_layout()\n",
    "#plt.savefig('../output/Figure_appendix_2.pdf', bbox_inches='tight')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8, 5, figsize=(18, 18), sharex=True, sharey=True)\n",
    "for i in range(8):\n",
    "    for j in range(5):\n",
    "        id_loc = i * 5 + j + 90\n",
    "        n_sample = len(Y[id_loc])\n",
    "        p_loc = np.linspace(-2, 2, 100)\n",
    "        pdf_V = wp.dgmm1d(p_loc, mu_test[id_loc], sigma_test[id_loc], pi_test[id_loc])\n",
    "        pdf_T = wp.dgmm1d(p_loc, mu_train[id_loc], sigma_train[id_loc], pi_train[id_loc])\n",
    "        ax[i, j].plot(p_loc, pdf_T, label='train', c='black', linestyle='-')\n",
    "        ax[i, j].plot(p_loc, pdf_V, label='test', c='black', linestyle='-.')\n",
    "        ax[i, j].set_title(str(year_select[id_loc]), fontsize=16) \n",
    "        #ax[i, j].legend(loc='upper right', fontsize=16)\n",
    "        ax[i, j].hist(Y[id_loc], bins=15, density=True, edgecolor='black', color='white')\n",
    "        ax[i, j].tick_params(labelsize=15)\n",
    "        #ax[i, j].set_xlabel('Degrees Fahrenheit', fontsize=12)\n",
    "fig.tight_layout()\n",
    "#plt.savefig('../output/Figure_appendix_3.pdf', bbox_inches='tight')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrain the model over the entire data set\n",
    "lr_list = [1e-1, 5e-2, 1e-2]\n",
    "K_mix = 3\n",
    "n_iter = 1000\n",
    "\n",
    "X_t, X_v, y_t, y_v = train_test_split(X, Q_mat, test_size=0.25, random_state=2020)\n",
    "loss_ = []\n",
    "iters_ = []\n",
    "for lr in lr_list:\n",
    "    print('Learning rate:', lr)\n",
    "    res_init = wp.WDL(X_t, y_t, X_v, y_v,\n",
    "                      q_vec=q_vec, K=K_mix, max_iter=n_iter, warm_up=1, max_depth=1, \n",
    "                      patience=5, lr=lr, random_state=2020)\n",
    "    iters_.append(res_init['iter_best'])\n",
    "    loss_.append(res_init['val_loss'][res_init['iter_best']])\n",
    "## choose the best params\n",
    "lr_best = lr_list[np.argmin(np.array(loss_))]\n",
    "iter_best = iters_[np.argmin(np.array(loss_))]\n",
    "print('Best:', lr_best, iter_best)\n",
    "## retrain the model over the training set\n",
    "res = wp.WDL(X, Q_mat, X, Q_mat, q_vec=q_vec, \n",
    "             K=K_mix, max_iter=iter_best, warm_up=1, \n",
    "             max_depth=1, early_stop=False, lr=lr_best, random_state=2020)\n",
    "\n",
    "v_lr = np.array([1] + [lr_best] * iter_best)\n",
    "\n",
    "## choose evaluation resolution\n",
    "n_evals = 100\n",
    "q_evals = np.arange(1, n_evals+1) / (n_evals+1)\n",
    "## initialize the parameters\n",
    "mu_partial = np.zeros((4, n_evals, K_mix))\n",
    "sigma_partial = np.zeros((4, n_evals, K_mix))\n",
    "pi_partial = np.zeros((4, n_evals, K_mix))\n",
    "time_start = datetime.now()\n",
    "print('Start:', time_start)\n",
    "for i in range(4):\n",
    "    for j in range(n_evals):\n",
    "        X_c = X.copy()\n",
    "        X_c[:, i] = np.quantile(X[:, i], q_evals[j])\n",
    "        alpha_temp = np.zeros((n_dist, K_mix))\n",
    "        mu_temp = np.zeros((n_dist, K_mix))\n",
    "        sigma_temp = np.zeros((n_dist, K_mix))\n",
    "        for k in range(K_mix):\n",
    "            alpha_temp[:, k] = wp.pred_boost(X_c, res['alpha'][k], lr_=v_lr, n_term=n_iter)\n",
    "            mu_temp[:, k] = wp.pred_boost(X_c, res['mu'][k], lr_=v_lr, n_term=n_iter)\n",
    "            sigma_temp[:, k] = np.exp(wp.pred_boost(X_c, res['sigma'][k], lr_=v_lr, n_term=n_iter))\n",
    "        pi_temp = np.exp(alpha_temp)\n",
    "        pi_temp = (pi_temp.T / np.sum(pi_temp, axis=1)).T\n",
    "        ## save the result\n",
    "        mu_partial[i, j] = np.mean(mu_temp, axis=0)\n",
    "        sigma_partial[i, j] = np.mean(sigma_temp, axis=0)\n",
    "        pi_partial[i, j] = np.mean(pi_temp, axis=0)\n",
    "print('Time:', datetime.now() - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the figure\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from scipy.stats import norm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "txt_size = 15\n",
    "alpha_lev = 0.5\n",
    "line_width = 0.5\n",
    "x_eval = np.linspace(-2, 2, 500)\n",
    "fig, ax = plt.subplots(4, 4, figsize=(20, 9))\n",
    "cm_raw = cm.get_cmap('coolwarm', 512)\n",
    "colormap =  ListedColormap(cm_raw(np.linspace(0.1, 1, 256)))\n",
    "## plot first row\n",
    "normalize = mcolors.Normalize(vmin=min(X[:, 0]), vmax=max(X[:, 0]))\n",
    "for j in range(n_evals):\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[0, j, 0], sigma_partial[0, j, 0]) * pi_partial[0, j, 0]\n",
    "    ax[0, 1].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 0], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[0, j, 1], sigma_partial[0, j, 1]) * pi_partial[0, j, 1]\n",
    "    ax[0, 2].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 0], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[0, j, 2], sigma_partial[0, j, 2]) * pi_partial[0, j, 2]\n",
    "    ax[0, 3].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 0], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "scalarmappaple.set_array([])    \n",
    "fig.colorbar(scalarmappaple, ax=ax[0, :], pad=0.01)\n",
    "ax[0, 0].plot(year_select, X[:, 0], c='black')\n",
    "ax[0, 0].set_ylabel('CO2', fontsize=txt_size)\n",
    "ax[0, 0].set_xticklabels([])\n",
    "ax[0, 1].set_xticklabels([])\n",
    "ax[0, 2].set_xticklabels([])\n",
    "ax[0, 3].set_xticklabels([])\n",
    "\n",
    "normalize = mcolors.Normalize(vmin=min(X[:, 1]), vmax=max(X[:, 1]))\n",
    "for j in range(n_evals):\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[1, j, 0], sigma_partial[1, j, 0]) * pi_partial[1, j, 0]\n",
    "    ax[1, 1].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 1], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[1, j, 1], sigma_partial[1, j, 1]) * pi_partial[1, j, 1]\n",
    "    ax[1, 2].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 1], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[1, j, 2], sigma_partial[1, j, 2]) * pi_partial[1, j, 2]\n",
    "    ax[1, 3].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 1], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "scalarmappaple.set_array([])    \n",
    "fig.colorbar(scalarmappaple, ax=ax[1, :], pad=0.01)\n",
    "ax[1, 0].plot(year_select, X[:, 1], c='black')\n",
    "ax[1, 0].set_ylabel('Solar', fontsize=txt_size)\n",
    "ax[1, 0].set_xticklabels([])\n",
    "ax[1, 1].set_xticklabels([])\n",
    "ax[1, 2].set_xticklabels([])\n",
    "ax[1, 3].set_xticklabels([])\n",
    "\n",
    "normalize = mcolors.Normalize(vmin=min(X[:, 2]), vmax=max(X[:, 2]))\n",
    "levels = np.percentile(X[:, 2], np.linspace(0,100,10))\n",
    "normalize = mcolors.BoundaryNorm(levels, 256)\n",
    "#normalize = mcolors.Normalize(vmin=-0.5, vmax=0)\n",
    "for j in range(n_evals):\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[2, j, 0], sigma_partial[2, j, 0]) * pi_partial[2, j, 0]\n",
    "    ax[2, 1].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 2], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[2, j, 1], sigma_partial[2, j, 1]) * pi_partial[2, j, 1]\n",
    "    ax[2, 2].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 2], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[2, j, 2], sigma_partial[2, j, 2]) * pi_partial[2, j, 2]\n",
    "    ax[2, 3].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 2], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "scalarmappaple.set_array(X[:, 2])    \n",
    "fig.colorbar(scalarmappaple, ax=ax[2, :], pad=0.01, format=tick.FormatStrFormatter('%.2f'))\n",
    "ax[2, 0].plot(year_select, X[:, 2], c='black')\n",
    "ax[2, 0].set_ylabel('Volcano', fontsize=txt_size)\n",
    "ax[2, 0].set_xticklabels([])\n",
    "ax[2, 1].set_xticklabels([])\n",
    "ax[2, 2].set_xticklabels([])\n",
    "ax[2, 3].set_xticklabels([])\n",
    "\n",
    "\n",
    "normalize = mcolors.Normalize(vmin=min(X[:, 3]), vmax=max(X[:, 3]))\n",
    "levels = np.percentile(X[:, 3], np.linspace(0,100,10))\n",
    "normalize = mcolors.BoundaryNorm(levels, 256)\n",
    "#normalize = mcolors.Normalize(vmin=-0.5, vmax=0)\n",
    "for j in range(n_evals):\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[3, j, 0], sigma_partial[3, j, 0]) * pi_partial[3, j, 0]\n",
    "    ax[3, 1].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 3], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[3, j, 1], sigma_partial[3, j, 1]) * pi_partial[3, j, 1]\n",
    "    ax[3, 2].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 3], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "    pdf_V = norm.pdf(x_eval, mu_partial[3, j, 2], sigma_partial[3, j, 2]) * pi_partial[3, j, 2]\n",
    "    ax[3, 3].plot(x_eval, pdf_V, c= colormap(normalize(np.quantile(X[:, 3], q_evals[j]))), lw=line_width, alpha=alpha_lev)\n",
    "scalarmappaple = cm.ScalarMappable(norm=normalize, cmap=colormap)\n",
    "scalarmappaple.set_array(X[:, 3])    \n",
    "fig.colorbar(scalarmappaple, ax=ax[3, :], pad=0.01, format=tick.FormatStrFormatter('%.2f'))\n",
    "ax[3, 0].plot(year_select, X[:, 3], c='black')\n",
    "ax[3, 0].set_ylabel('El Nino', fontsize=txt_size)\n",
    "ax[3, 0].set_xlabel('Year', fontsize=txt_size)\n",
    "ax[3, 1].set_xlabel('Component I', fontsize=txt_size)\n",
    "ax[3, 2].set_xlabel('Component II', fontsize=txt_size)\n",
    "ax[3, 3].set_xlabel('Component III', fontsize=txt_size)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax[i, j].tick_params(labelsize=12)\n",
    "plt.savefig('../output/Figure_06.pdf', bbox_inches='tight')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
